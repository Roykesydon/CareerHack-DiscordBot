from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline

from VDB_API.agent.api_executor import execute_api_call
from VDB_API.agent.prompt_generation import generate_planning_prompt
from VDB_API.utils.config import CONTINUE_SEARCH_WORD
from VDB_API.utils.file_processor import add_unique_docs

# model = ChatOpenAI(temperature=0)


async def update_new_processing_message(response, processing_message):
    action_text_dict = {
        "DCBA_LLM": "ğŸ” æ­£åœ¨ç”¨ DCBA æ©Ÿå™¨äººæŸ¥æ‰¾...",
        "Search": "ğŸ” æ­£åœ¨ç”¨ Google æœå°‹...",
        "Arxiv": "ğŸ” æ­£åœ¨ Arxiv ä¸Šæœå°‹...",
        "LLMMathChain": "ğŸ” æ­£åœ¨ç”¨æ•¸å­¸æ©Ÿå™¨äºº...",
    }
    action = None
    query = None

    response_split_list = response.split()
    response_split_list = [x.strip() for x in response_split_list]

    # get action
    try:
        action_index = response_split_list.index("Action:")
        if action_index != -1:
            action = response_split_list[action_index + 1]
    except:
        pass

    # get query
    try:
        action_input_index = response_split_list.index("Input:")
        if action_input_index != -1:
            query = "".join(response_split_list[action_input_index + 2 :])
            query = query.replace('"', "")
            query = query.replace("}", "")
            query = query.replace("{", "")

    except:
        pass

    if action is not None and action in action_text_dict.keys() and query is not None:
        return_message = action_text_dict[action]
        # replace ...
        return_message = return_message.replace("...", "ï¼š")
        await processing_message.edit(content=return_message + query)
        return

    if action is not None and action in action_text_dict.keys():
        return_message = action_text_dict[action]
        await processing_message.edit(content=return_message)
        return


async def llm_agent(query, model, selected_tools, processing_message=None):
    docs = []
    prompt = generate_planning_prompt(selected_tools, query)  # å»ºç«‹è¨­å®šå¥½çš„ prompt
    stop = ["Observation:", "Observation:\n"]
    if isinstance(model, HuggingFacePipeline):
        response = model.invoke(prompt, stop=stop)  # ç”Ÿæˆ response
    else:
        response = model.invoke(prompt, stop=stop).content

    if processing_message is not None:
        await update_new_processing_message(response, processing_message)
    for i in range(3):
        if "Final Answer:" in response:  # è‹¥å›æ‡‰ä¸­å‡ºç¾ "Final Answer:" å‰‡åœæ­¢
            break
        api_output, tmp_docs = execute_api_call(
            selected_tools,
            response,
        )  # é€é parser è§£æ response ä¸¦åŸ·è¡Œå°æ‡‰çš„ api
        docs = add_unique_docs(docs, tmp_docs)
        api_output = str(api_output)  # å°‡ api è¼¸å‡ºè½‰ç‚ºå­—ä¸²
        print(
            "\033[32m"
            + response
            + "\033[0m"
            + "\033[34m"
            + "Observation: "
            + api_output
            + "\033[0m"
        )
        if "no tool founds" == api_output:
            break
        if CONTINUE_SEARCH_WORD in api_output:
            return "æŠ±æ­‰æˆ‘ç„¡æ³•è§£æ±ºæ‚¨çš„å•é¡Œï¼Œä½†æ‚¨å¯ä»¥é»æ“Šä»¥ä¸‹åƒè€ƒæŒ‰éˆ•ï¼Œç²å¾—æ›´å¤šè³‡è¨Š", docs

        print("-" * 20)
        prompt = (
            prompt + response + "Observation: " + api_output
        )  # å°‡ Api è¼¸å‡ºæ”¾åˆ° observation ä¸­ï¼Œä¸¦æ›´æ–° prompt

        if isinstance(model, HuggingFacePipeline):
            response = model.invoke(
                prompt, stop=stop
            )  # ç”Ÿæˆæ–°çš„ responseï¼Œç›´åˆ°å‡ºç¾ "Final Answer:"
        else:
            response = model.invoke(prompt, stop=stop).content
        print(f"response: {response}")
        if processing_message is not None:
            await update_new_processing_message(response, processing_message)
    else:
        prompt = prompt + response + "Thought: I now know the final answer"
        response = model.invoke(prompt).content
    print("\033[32m" + response + "\033[0m")
    print("-" * 20)
    final_answer_index = response.rfind("Final Answer:")
    additional_length = len("Final Answer:") if final_answer_index != -1 else 0
    final_answer = response[final_answer_index + additional_length :].strip()
    return final_answer, docs


# query = "æˆ‘ç¾åœ¨162å…¬åˆ†ï¼Œè«‹å•æˆ‘çš„èº«é«˜å’Œå°ç©é›»å“¡å·¥èº«é«˜å¹³å‡ç›¸å·®å¤šå°‘ï¼Ÿ"
# query = 'å°ç©é›»ä¸€å¹´æœ‰å¤šå°‘å¤©çš„å‡æœŸï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
# query = 'å°ç©é›»ä¸€å¹´æœ‰å¤šå°‘å¤©çš„å‡æœŸï¼Ÿ'
# query = 'è«‹æ ¹æ“šç¶²è·¯è¨Šæ¯ï¼Œè«‹å•å°ç©é›»ä¸€å¹´æœ‰å¤šå°‘å¤©çš„å‡æœŸï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
# query = 'è«‹å•æˆ‘ä¸€å¹´æœ‰å¤šå°‘å¤©çš„å‡æœŸï¼Ÿ'
# query = "è«‹å•å“¡å·¥ä¸€å¹´æœ‰å¤šå°‘å¤©çš„å‡æœŸï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚"
# query = 'è«‹å•å°ç©é›»æä¾›å“ªäº›ç”¢å“å’Œæœå‹™ï¼Ÿ'

# query = 'è«‹æ ¹æ“šç¶²è·¯ä¸Šçš„è³‡æ–™ï¼Œå‘Šè¨´æˆ‘å°ç©é›»æä¾›å“ªäº›ç”¢å“å’Œæœå‹™ï¼Ÿ'
# query = 'å°ç©é›»æ˜¯ä»€éº¼å…¬å¸ï¼Œä»–å€‘ä¸»è¦å¾äº‹ä»€éº¼æ¥­å‹™ï¼Ÿ'
# query = 'è«‹ä»‹ç´¹å°ç©é›»é€™é–“å…¬å¸ï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
# query = 'è«‹æ ¹æ“šç¶²è·¯ä¸Šçš„è³‡æ–™ï¼Œä»‹ç´¹å°ç©é›»é€™é–“å…¬å¸ï¼Œä¸¦ä»¥ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
# query = 'å°ç©é›»æ˜¯åŠå°é«”è£½é€ æ¥­çš„é ˜å…ˆè€…å—ï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”'
# ä»–å€‘æœ‰å“ªäº›å…ˆé€²è£½ç¨‹æŠ€è¡“ï¼Ÿ
# query = 'è«‹å•æœ€å…ˆé€²çš„è£½ç¨‹åœ¨å“ªå€‹å» å€ï¼Ÿè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚'
